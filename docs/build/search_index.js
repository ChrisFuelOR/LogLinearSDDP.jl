var documenterSearchIndex = {"docs":
[{"location":"cuts/#How-cuts-are-represented-in-the-code","page":"How cuts are represented in the code","title":"How cuts are represented in the code","text":"As described in our paper, for any t = 2ldotsT and any incumbent (x_t-1 xi_t-pt-1) cuts in loglinearSDDP can be expressed by formula\n\nbeginequation\nbeginaligned\nmathcalQ_t(x_t-1 xi_t-pt-1) geq beta_barr t^top x_t-1 + sum_tau = t^T sum_ell=1^L_tau bigg( alpha_barr t ell^(tau) prod_k=t-p^t-1 prod_m=1^L_k xi_km^Theta(ttauellmk) bigg)\nendaligned\nendequation\n\nwith coefficients defined by\n\nbeginequation\nbeginaligned\nbeta_barr tj^top = - big( pi_tj^* big)^top T_t-1 \nalpha_barr t ell j^(t) = pi_t ell j^* e^gamma_t ell e^psi_t ell eta_t ell^(j) \nalpha_barr t ell j^(tau) = Big( sum_r in R_t+1 rho^*_rtj alpha_rt+1ell^(tau) Big) prod_nu=1^L_t e^gamma_t nu Theta(t+1tauellnut) e^psi_t nu eta_t nu^(j) Theta(t+1tauellnut) \nquad tau = t+1ldotsT \nendaligned\nendequation\n\nfor t=2ldotsT ell = 1ldotsL_tau and j=1ldotsq_t,\n\nbeginequation\nbeginaligned\nbeta_barr t = sum_j=1^q_t p_tj beta_barr tj quad quad alpha_barr t ell^(tau) = sum_j=1^q_t p_tj alpha_barr t ell j^(tau) quad tau = tldotsT\nendaligned\nendequation\n\nand\n\nbeginequation\nbeginaligned\nTheta(ttellmk) = phi_t ell m^(t-k) \nTheta(ttauellmk) = begincases sum_nu=1^L_t big( phi_t nu m^(t-k) Theta(t+1tauellnut) big)  quad + Theta(t+1tauellmk) quad  textif  k geq t-p+1  sum_nu=1^L_t big( phi_t nu m^(p) Theta(t+1tauellnut) big)  textif  k = t-p endcases \nendaligned\nendequation\n\nfor t=2ldotsT, tau = t+1ldotsT, k=t-pldotst-1, ell=1ldotsL_tau and m=1ldotsL_k. barr denotes the index of the new cut.\n\nFor detailed explanations of all the terms in (1)-(4) and how they are derived, we refer to our paper.\n\nWhen a subproblem for a specific scenario is solved within loglinearSDDP, the cut intercepts of all existing cuts are adapted to said scenario by re-evaluating the second summand in (1) accordingly.\n\nIn the code, the required information to handle these types of cuts is stored in structs of type Cut, as known from SDDP.jl. However, the struct is adjusted to satisfy the special form of the cuts above.\n\nusing JuMP\n\nmutable struct Cut\n    coefficients::Dict{Symbol,Float64}\n    deterministic_intercept::Float64\n    stochastic_intercept_tight::Float64\n    intercept_factors::Array{Float64,2}\n    trial_state::Dict{Symbol,Float64}\n    constraint_ref::Union{Nothing,JuMP.ConstraintRef}\n    cut_intercept_variable::Union{Nothing,JuMP.VariableRef}\n    non_dominated_count::Int64\n    iteration::Int64\nend\n\nIts fields are defined as follows: *\tcoefficients: This is the cut gradient vector beta. It can be computed using the values of certain dual variables. *\tdeterministic_intercept: This value is used to account for the contribution of deterministic constraints to the intercept. Handling them as coupling constraints is unnecessary from a memory perspective, but not taking them into account leads to a wrong intercept overall. *\tstochastic_intercept_tight: This is the value of the full intercept at the incumbent, i.e. where the cut is constructed and tight. This is merely used for checks and to compute other values. It may also be used for cut selection purposes in the future. *\tintercept_factors is not a scalar intercept as in standard SDDP, but a matrix of intercept factors alpha_barr t ell^(tau) for each tau=tldotsT and each component ell of the process (see (3)). *\ttrial state; This is the incumbent where the cut is constructed. *\tcut_constraint: This refers to the cut constraint in the JuMP model. *\tcut_intercept_variable: This refers to an artificial variable in the JuMP model which is fixed to the cut intercept for a given, specific scenario. *\tnon_dominated_count is required for cut selection purposes. Cut selection is not supported yet but might be considered in the future. *\titeration: This value stores the iteration number in which the cut was constructed, which often coincides with index barr above. This is used for logging and analyses.\n\nnote: Remark\nThe exponents in (4) only have to computed once as they do not change between cuts,scenarios and iterations.Once the alpha factors in (2) and (3) are computed when a new cut is generated, they are fixed cut coefficients.\nWhen cut intercepts are re-evaluated for a given scenario, then the scenario-specific factorsprod_k=t-p^t-1 prod_m=1^L_k xi_km^Theta(ttauellmk)in (2) are the same for all cuts, so they only have to be re-evaluated once for all cuts. Only the factors alpha_barr t ell^(tau) are specific to each cut.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/markov/#Special-remarks-to-MC-SDDP","page":"Special remarks to MC-SDDP","title":"Special remarks to MC-SDDP","text":"When using MC-SDDP, the model does not have to be defined manually (as in hydrothermal_model.jl), but is read from  (12_0)_100.problem.json. We have made sure that the model specified there exactly matches the model considered in hydrothermal_model.jl and hydrothermal_model_linearized.jl.\n\nAdditionally, also the lattice (Markov chain) data is read from (12_0)_100.lattice.json. Reading this data can take some time, which is not included in the overall run time, though, to allow a fair comparison with other variants of SDDP.\n\nThe problem and lattice data are taken from the MSPLib.\n\nRuns for MC-SDDP are conducted using run_model_markov.jl. The structure is very similar to run_model.jl. An important difference is that we can specify a forward_pass_model. When it is set to lattice, the scenario lattice itself is used for sampling within SDDP. Otherwise, we may also use inflows from the log-linear or linearized AR models (similar to the out-of-sample simulations). As these inflow values may not be included in the discrete lattice, we use function closest_nod from markov.jl to compute the closest lattice node to an inflow realization.\n\nWhen solving a particular subproblem in the forward pass of SDDP, we use the exact sampled inflow value in the RHS. However, we then use the closest lattice node to decide which node is visited next in the forward pass. This is a standard approach in the literature when using scenario lattices combined with out-of-sample data.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/setting_up_process/#Setting-up-the-autoregressive-processes","page":"Setting up the autoregressive processes","title":"Setting up the autoregressive processes","text":"For both, log-linear and linearized AR processes, the processes are set up using functions in set_up_ar_process.jl.\n\nAs inputs for this set-up three sources are used. *\tFor each reservoir system (N, S, NE, SE) and for each month, a log-linear AR model was fitted using the historical data (for more details, see [TODO]). The estimates are stored in folder AutoregressivePreparation and then bic_model (or custom_model) in some txt-files (for the linear case it is LinearizedAutoregressivePreparation). Each txt-file contains the months, the lag order, the process intercept, the process coefficients, the factor multiplied with the error term and the standard error from the estimation. *\tFor each reservoir system, some historical inflow values are provided in the same folder in the txt-file history_nonlinear.txt. It contains a sufficient number of time steps for the lag order of the processes. *\tFor each reservoir system and each stage, 100 realizations of the stagewise independent noise term eta_t are provided in the same folder in the txt-file scenarios_nonlinear.txt.\n\nIn the set-up process, first the above data is read from the source files. Then it is used to create structs of type AutoregressiveProcessStage and AutoregressiveProcess.\n\nAs loglinearSDDP requires a constant lag order, the maximum over all reservoirs and months is used.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"model_assumptions/#Model-assumptions","page":"Model assumptions","title":"Model assumptions","text":"loglinearSDDP is designed for multistage stochastic linear problems with log-linear autoregressive uncertainty in the right-hand side (RHS). For simplicity, we assume that we face only uncertainty of data in the RHS, even though in theory also stagewise independent uncertainty in other problem components is allowed.\n\nAs described in our paper, this type of multistage problem can be described by coupled subproblems for stages t=1ldotsT, each of them of the form\n\nbeginaligned\nQ_t(x_t-1 xi_t) =\nbegincases\nbeginaligned\nmin_x_t c_t^top x_t + mathcalQ_t+1(x_t xi_t) \ntextst T_t-1 x_t-1 + W_t x_t = xi_t \nx_t geq 0\nendaligned\nendcases\nendaligned\n\nwhere x_t is the decision variable, xi_t denotes the uncertain data, Q_t is the so-called value function for stage t and mathcalQ_t+1 is the so-called expected value function for stage t+1.\n\nCurrently, the theory and the code are restricted to problems satisfying the following properties: *\tfinitely many stages T *\tlinear constraints and objective *\tcontinuous decision variables *\tlog-linear autoregressive uncertainty in the RHS *\tfinite support of the uncertainty *\tdeterministic first stage *\tuncertainty is exogeneous (not decision-dependent) *\texpected value is considered in the objective (no other risk measures) *\tno usage of believe or objective states compared to SDDP.jl\n\nAdditionally, so far our code is restricted to using a single-cut approach (see parameter SINGLE_CUT in the code), where the expected value functions mathcalQ_t are approximated by one set of cuts per stage. A multi-cut approach (MULTI_CUT), where for each stage value functions Q_t for different scenarios are approximated by separate sets of cuts, is not supported yet.\n\nMost of these assumptions are in line with what is known from using SDDP.jl. The major difference is that in our case the uncertainty has to be modeled differently to account for its autoregressive character. More details on this are provided in Modeling log-linear AR processes\n\nImportantly, contrary to other approaches in the literature, loglinearSDDP does not require the user to model an explicit state expansion for the given problem to take the history of the AR process into account. Instead, tailored cut formulas are used to adapt the cut intercept to a scenario at hand. We provide an example for a hydrothermal scheduling problem below to further clarify this [TODO].\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/logging/#Interpreting-the-logging-results","page":"Interpreting the logging results","title":"Interpreting the logging results","text":"For each run in our experiments, several outputs are logged. In general, there are three types of output files:\n\n*\tA main logging file called LogLinearSDDP.log (or LinearSDDP.log or MC-SDDP.log depending on the chosen variant of SDDP). *\tSome files which log simulation outputs (e.g. inflows, volumes, costs for different reservoirs). They are named something like bic_model_custom_model_volumes_N.txt. This means that the policy obtained using the bic_model is simulated with out-of-sample data from the custom_model and that the file contains the volumes of reservoir N. These outputs are required for some analyses in the paper. *\tA file inflows.txt that logs inflows values. This was used for (a) checks if inflows were indeed consistent between out-of-sample simulations for different policies and (b) to generate the inflow data input files for run_markov.jl.\n\nWe explain the main logging file LogLinearSDDP.log in more detail. It consists of the following elements:\n\n*\tA section borrowed from SDDP.jl logging some general information about the multistage problem *\tA section printing the main model run parameters     *\tthe file path, the optional run description, the date and time of the run     *\tthe main parameters defined in the ProblemParams struct (problem size, number of realizations per stage)     *\tthe main parameters defined in the AlgoParams struct (sampling seed, stopping rules)     *\tthe main properties of the uncertainty model (the used model approach, the lag order, the dimension) *\tA section logging information from the SDDP iterations. Each row contains     *\tthe iteration number     * the deterministic lower bound     *\tthe simulated upper bound and the gap (both are not relevant for our experiments)     *\tthe total time and the time for the specific iteration     *\tthe total number of cuts created and the active number of cuts (which differs if a cut selection scheme is used) *\tA section borrowed from SDDP.jl summarizing the SDDP results     *\ttotal time     *\tstopping status     *\tbest deterministic lower bound     *\tsimulated upper bound and confidence interval *\tA table summarizing timing and memory allocation information for different steps of the algorithm *\tA section containing the results of in-sample and out-of-sample simulations after SDDP has terminated. In each case the information contains     *\tthe used uncertainty model     *\tthe deterministic lower bound     *\tthe simulated upper bound and a confidence interval\n\nNote that each simulation run accounts for two logs of simulation results, one including costs for all 120 stages and one including only costs for the first 60 stages (to remove the end-of-horizon effect). The latter are reported in the paper.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/experiment_description/#Computational-experiments-for-a-hydrothermal-scheduling-problem","page":"Computational experiments for a hydrothermal scheduling problem","title":"Computational experiments for a hydrothermal scheduling problem","text":"","category":"section"},{"location":"example/experiment_description/#The-hydrothermal-scheduling-problem","page":"Computational experiments for a hydrothermal scheduling problem","title":"The hydrothermal scheduling problem","text":"To test our proposed variant of SDDP, loglinearSDDP, and compare it to existing variants, we conducted experiments on a hydrothermal scheduling problem from the literature [CITE]. The multistage problem has T=120 stages, uses 4 aggregated reservoirs, 95 generators and is based on real data from the Brazilian hydro power system. The hydro inflows into the reservoirs are considered stochastic and modeled by an autoregressive process.\n\nThe data for this problem is available in the MSPLib by Bonn Kleiford Seranilla and Nils Löhndorf.","category":"section"},{"location":"example/experiment_description/#Our-computational-tests","page":"Computational experiments for a hydrothermal scheduling problem","title":"Our computational tests","text":"In our paper we compare three different variants of SDDP and the policies that we obtain using these variants. *\tloglinearSDDP: Our version of SDDP from this repository. It is suited for uncertainty modeled by log-linear AR processes. *\tSDDP with linearized AR processes: Uses standard SDDP from SDDP.jl. Requires to linearize the log-linear AR process beforehand. *\tMarkov-chain SDDP: Uses the Markov-chain SDDP (MC-SDDP) variant included in SDDP.jl. It requires a Markov-chain approximation (scenario lattice) of the uncertain inflows. This lattice is provided in the MSPLib as well.\n\nEach of these three cases requires slightly different set-ups and function calls due to the differences in the SDDP algorithms as well as the handling of the uncertain data. For this reason there exist three variants of the same functionality several times in our code.\n\nIn this documentation, we mostly focus on how to run the loglinearSDDP variant but highlight important differences for the other variants when it feels required.\n\nIn addition to SDDP, we used different models for the uncertain data in the RHS. *\tcustom_model (LOG-1 in the paper): log-linear AR process with lag order 1 *\tbic_model (LOG-BIC in the paper): log-linear AR process with lag order estimated using the Bayesian Information Criterion *\tfitted_model (LIN-FIT in the paper): linearized AR process with lag order 1 fitted using our code *\tshapiro_model (LIN-SHA in the paper): linearized AR process with lag order 1 and parameters from the literature\n\na scenario lattice lattice for MC-SDDP\n\n*\thistorical data historical\n\nMore details on how we fitted these models is provided in [TODO].\n\nWe use uncertain data obtained from the log-linear models within loglinearSDDP to train our policy, and uncertain data obtained from the linearized models within standard SDDP to train our policy. For MC-SDDP we use either data from the lattice itself, from log-linear models or from the linearized models.\n\nAfter running SDDP, we evaluate all obtained policies by running simulations using out-of-sample data from all four stochastic processes as well as historical data.\n\n(Image: Overview of simulations)\n\nNote that the code for these simulations (e.g. setting up the uncertainty model and data, simulating new data etc.) has to be catered to the specific variant of the uncertainty, which is why our code contains various slightly different files and methods dealing with simulation. For more details on this, see [TODO].\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/running_the_model/#Running-computations","page":"Running computations","title":"Running computations","text":"To run loglinearSDDP, we use file run_model.jl. It allows to specify several runs with different parameter configurations. Those runs are executed one after each other.\n\nFor each run, we provide a random seed (used for the sampling in the forward pass), a specific log-linear model for the uncertain data and additional log-linear and linearized models that are used for the out-of-sample simulations after SDDP has terminated.\n\nThe run_model function has the following general structure: *\tDefine main model parameters. For out tests, we always used 120 stage, 100 realizations per stage and 2000 scenarios in the simulations. *\tSet the file path (this has to be adjusted to the user’s system when trying to reproduce our results). *\tUse the previously defined parameters to set up structs of type ProblemParams and AlgoParams (see definitions in Further parameters). *\tDefine a simulation regime that will be used for the in-sample simulation after SDDP has terminated. *\tSet up the log-linear AR process by calling function set_up_ar_process_loglinear from set_up_ar_process.jl. *\tCall model_definition from hydrothermal_model.jl to construct the multistage optimization problem. This requires to pass the previously defined process as an argument. *\tCall the train_loglinear function from algorithm.jl to start running SDDP. *\tAfter running SDDP, perform several simulations. For details, see [TODO]]. *\tThe function extended_simulation_analysis is used to analyze and log parts of the simulation output.\n\nNOTE: For running different variants of SDDP, the procedure is very similar (see run_model_linearized.jl or run_model_markov.jl).\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"params/#Further-parameters","page":"Further parameters","title":"Further parameters","text":"There are two more structs that are relevant for the user when running loglinearSDDP.","category":"section"},{"location":"params/#ProblemParams","page":"Further parameters","title":"ProblemParams","text":"struct ProblemParams\n    number_of_stages::Int64\n    number_of_realizations::Int64\n    tree_seed::Union{Nothing,Int}\n    gurobi_coupling_index_start::Union{Nothing,Int}\n    gurobi_cut_index_start::Union{Nothing,Int}\n    gurobi_fix_start::Union{Nothing,Int}\nend\n\nThis struct is used to store some parameters of the test problem that is solved. This is mainly used for logging purposes.\n\nIts fields are defined as follows: *\tnumber_of_stages: number of stages of the multistage problem *\tnumer_of_realizations: number of realizations of the stagewise independent noise eta_t *\ttree_seed: seed for the scenario tree generation *\tgurobi parameters: these parameters refer to Gurobi-internal indices of coupling constraints, cut constraints and cut intercept variables; if they are known, they can be provided by the user to speed up parts of the computations (see more details below)\n\nNote that the size of eta in AutoregressiveProcessStage should match number_of_realizations defined in ProblemParams.","category":"section"},{"location":"params/#AlgoParams","page":"Further parameters","title":"AlgoParams","text":"using LogLinearSDDP, SDDP\n\nmutable struct AlgoParams\n    stopping_rules::Vector{SDDP.AbstractStoppingRule}\n    simulation_regime::LogLinearSDDP.AbstractSimulationRegime\n    cut_selection::Bool\n    print_level::Int64\n    log_frequency::Int64\n    log_file::String\n    run_numerical_stability_report::Bool\n    numerical_focus::Bool\n    silent::Bool\n    forward_pass_seed::Union{Nothing,Int}\n    run_description::String\n    model_approach::Symbol\nend\n\nThis struct is used to store some parameters that control the SDDP algorithm that is used to solve the model. Many of these parameters are the same that the train function in SDDP.jl uses as arguments.\n\nIts fields are defined as follows: *\tstopping_rules: A vector of different SDDP stopping rules (same as for SDDP.jl). *\tsimulation_regime: Defines which type of simulation should be used for the in-sample simulations (see [TODO]). *\tcutselection: Controls if a cut selection scheme should be applied. Note that this is not supported yet but might be added in the future, so we added a placeholder. *\t`printlevel,logfrequency,logfile: As fortrainin SDDP.jl, these parameters control the logging of the SDDP output *runnumericalstabilityreport: As fortrainin SDDP.jl, this parameter controls the generation of numerical stability reports. Note that this is only a placeholder so far and should always be set toFalse. *numericalfocus: If set toTrue, Gurobi will set its numerical_focus parameter toTrue, which may help in case of numerical issues but will also slow down the solver. *silent: Controls if the solver output is printed on the console. *\tforward_pass_seed: seed that is used for sampling realizations of the stagewise independent noise ηₜ in the forward pass of SDDP; note that this differs from the tree_seed in ProblemParams which is used to TODO *rundescription: Allows the user to add some description to a specific model run that is used in the log-file. *modelapproach`: For logging purposes, stores which concrete model is used for the stochastic process (i.e. LOG-BIC, LOG-1, LIN-FIT or LIN-SHA for our hydrothermal scheduling problem, see [TODO]).\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"uncertainty/#Modeling-log-linear-AR-processes","page":"Modeling log-linear AR processes","title":"Modeling log-linear AR processes","text":"The code is designed to deal with uncertainty in the RHS of the multistage problem that can be modeled by an autoregressive stochastic process that is described by formula\n\nbeginequation\nlog(xi_t ell) = gamma_t ell + sum_k=1^p sum_m=1^L_t-k phi^(k)_t ell m log(xi_t-k m) + psi_t ell eta_t ell\nendequation\n\nHere, gamma phi and psi are vectors and matrices of process coefficients that have to be estimated, p is the lag order and L is the dimension. We call these processes log-linear autoregressive processes, as they are linear functions in the natural logarithm of the process history.\n\nIn LogLinearSDDP.jl, the definition of the log-linear AR process for a given model can be provided by the user using the AutoregressiveProcess and AutoregressiveProcessStage structs (both are defined in the file src/typedefs.jl).","category":"section"},{"location":"uncertainty/#AutoregressiveProcess","page":"Modeling log-linear AR processes","title":"AutoregressiveProcess","text":"using LogLinearSDDP\n\nstruct AutoregressiveProcess\n    dimension::Int64\n    lag_order::Int64\n    parameters::Dict{Int64,LogLinearSDDP.AutoregressiveProcessStage}\n    history::Dict{Int64,Vector{Float64}}\n    simplified::Bool\nend\n\nThis struct can be used to store some general information on the stochastic process and its required history (lagged values xi_t-k m).\n\nThe fields of AutoregressiveProcess are defined as follows:\n\n*\tdimension: Int64 which defines the dimension of the random process; denoted by L in the paper *\tlag_order: Int64 which defines the lag order of the random process (same for each component and stage); denoted by p in the paper *\tparameters: Dict containing the stage-specific data of the process. The key is the stage and the value is the actual data struct of type AutoregressiveProcessStage; one-dimensional with component t *\thistory: Dict containing the historic values of the process (including stage 1). The key is the stage and the value is a vector of index ℓ (alternatively, a tuple could be used). *\tsimplified: If true, there are no dependencies between different process components (i.e. spatial dependencies). This allows to simplify some computations in our code. Referring to the paper, the process formula becomes\n\nbeginequation\nlog(xi_t ell) = gamma_t ell + sum_k=1^p phi^(k)_t ell log(xi_t-k ell) + psi_t ell eta_t ell\nendequation\n\nnote: Remarks\nWe assume that the lag order p is the same for all stages and components. Otherwise the cutformulas become way more sophisticated (see paper). In practice, different components and stages  #     may require different lag orders, for instance in SPAR models. If a stage-component combination requires less lags than globally defined, we can set the ar_coefficients for excessive lags to 1, so that they do not have any effect.In contrast to the paper - for this code we also assume that the dimension L of the process #     is the same for all stages.This allows us to accelerate nested loops in the code with tools that do not allow for indices to be dependent on each other. This is a very natural assumption in practice. For instance, in our hydrothermal scheduling example, we have the same number of reservoirs for each stage.We assume the first-stage data to be deterministic. Therefore, it should be included inhistory instead of parameters.","category":"section"},{"location":"uncertainty/#AutoregressiveProcessStage","page":"Modeling log-linear AR processes","title":"AutoregressiveProcessStage","text":"struct AutoregressiveProcessStage\n    intercept::Vector{Float64}\n    coefficients::Array{Float64,3}\n    psi::Vector{Float64}\n    eta::Vector{Any}\n    probabilities::Vector{Float64}\nend\n\nThis struct can be used to store the specific parameters of the log-linear autoregressive process for a particular stage t. Note that parameters can be defined separately for each component ell (componentwise process definition, see our paper).\n\nThe fields of AutoregressiveProcess are defined as follows:\n\n*\tintercept: Vector containing the intercepts of the process; one-dimensional with component ell; denoted by gamma in the paper *\tcoefficients: Array containing the coefficients of the process; three-dimensional with components ell, m and lag k; denoted by phi in the paper *\tpsi: Vector containing the pre-factor for eta in the process formula; one-dimensional with component ell; denoted by psi in the paper *\teta: Vector containing the stagewise independent realizations of the error term; denoted by eta in the paper     * Note that this information is merely stored in the struct for logging purposes and for setting up the optimization problem, but is not used in the actual algorithm.     * Each element of the vector should be a vector, tuple or named tuple of size ell in order to store values for different process components. This requirement is standard for SDDP.jl as well. *\tprobabilities: Probabilities related to eta (optional); note that this information is merely stored in the struct for logging purposes\n\nWe provide an example on how to set up a specific log-linear AR process for the hydrothermal scheduling problem from our paper in [TODO].\n\nnote: Remark\nNote that similar structs (LinearAutoregressiveProcessStage and LinearAutoregressiveProcess) #     are also defined in file set_up_ar_process.jl for our hydrothermal scheduling problem when  #     using linearized AR processes instead of log-linear AR processes.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/simulations/#Simulations","page":"Simulations","title":"Simulations","text":"We run several simulations (in-sample and out-of-sample) after SDDP has terminated.\n\nEach simulation run consists of the following three ingredients: *\tCreate a sampling scheme from the ones available in SDDP.jl (InSampleMonteCarlo, OutOfSampleMonteCarlo, Historical)     *\tFor InSampleMonteCarlo, we sample from the realizations passed in the model formulation.     *\tFor OutOfSampleMonteCarlo, we pass a function to the sampling scheme which defines how we sample realizations later.     *\tFor Historical, we pass a list of historical realizations to the sampling scheme from which we sample later. *\tCreate an struct of type LogLinearSDDP.Simulation which stores this sampling scheme as well as the number of replications (and optionally a random seed). *\tCall a simulation function that uses the information from this struct to execute the simulation. It is responsible for calling the correct sample_scenario method and passing the correct sampling scheme information.\n\nImportantly, for each variant of SDDP that we ran and the uncertainty model that we want to use for the simulation, these steps have to be adjusted accordingly.\n\nFor instance, if we want to use data from the lo-linear models to simulate the policy that we obtained when running standard SDDP with inflows from the linearized models, then we have to make sure that the out-of-sample inflows from the log-linear models are provided in a way that matches the model formulation and the parameterize function defined in hydrothermal_model_linearized.jl.\n\nFor this reason, our code contains a lot of variations of the same functions. An overview is given in the table below.\n\n(Image: Overview of simulations)\n\nNote that for out-of-sample simulations of Markov-chain SDDP policies, the inflows are not sampled during the simulation but read from inflow files in folder PreparationMarkov which have been generated using the log-linear and the linearized AR processes in advance.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"usage/#How-to-use-the-code","page":"How to use the code","title":"How to use the code","text":"","category":"section"},{"location":"usage/#Setting-up-the-repository","page":"How to use the code","title":"Setting up the repository","text":"Note that this code was developed a few years ago and only tested on Julia 1.9.2 and with SDDP.jl 1.6.6. From today’s perspective, the Julia version and many of the packages that are used may be outdated.\n\nWe have not managed yet to update the code in order to guarantee compatibility with all newer package versions. However, when we used Pkg.instantiate() to set up our repository on a different machine, we have only experienced some issues with newer versions of SDDP.jl. For SDDP.jl it seems that at least the set_objective function has changed compared to the one we use.\n\nFurther note that running loglinearSDDP for our hydrothermal scheduling example (see [TODO]) requires an installation of Gurobi together with a valid Gurobi license. If this requirement is not satisfied, the solver has to be changed in the run-files and a few adjustments have to be made in the code.","category":"section"},{"location":"usage/#Running-the-code","page":"How to use the code","title":"Running the code","text":"Once the repository is set up, the code for our hydrothermal scheduling problem can be run using the following steps.\n\nOpen Julia (1.9).\nUse cd to navigate to the repository directory.\nGo to package mode and execute activate ..\nIt is possible that the Gurobi path has to be made known. If so, use ENV[\"GUROBI_HOME] = [path] where [path] is the file path of your Gurobi installation.\nIt is possible that the Gurobi package has to be built for first usage. If so, use package mode and build Gurobi.\nUse cd to navigate to examples/Hydrothermal.\nExecute run_model.jl (or other run-files) with include(\"run_model.jl\").\n\nBefore using these steps, you have to make sure that the filepaths in `runmodel.jl,runmodellinearized.jl,runmodelmarkov.jl,markov.jlandcreateinflowsforMarkovloglinear.jlare set as intended. The places are highlighted byTODO` in the code.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/model/#Defining-the-multistage-problem","page":"Defining the multistage problem","title":"Defining the multistage problem","text":"The hydrothermal scheduling model itself is known from the literature [CITE]. The specific model formulation and data is taken from the MSPLib. The deterministic demand data is stored in demand.csv.\n\nThe model is defined using the JuMP.jl and SDDP.jl packages in file hydrothermal_model.jl.\n\nCompared to standard models solved with SDDP.jl the main difference is how the uncertain data in the model is parameterized. *\tRecall that loglinearSDDP does not require the user to model an explicit state expansion for the given problem to take the history of the AR process into account. *\tThe realizations that are stored in node.noise_terms within SDDP and that we sample from in the forward pass are defined by the stagewise independent error term eta_t of the log-linear AR process only. *\tHowever, with our adjustments in algorithm.jl and sampling_schemes.jl we make sure that in parameterize, the omega values that the inflow variables are actually fixed to are computed using the whole log-linear process formula given a particular realization of eta. *\tThe first-stage data is considered deterministic, so there should be only one realization (for each dimension of the uncertainty). This can always be set to 0. *\tNote that the parameterize statement contains some print commands. These are optional and can be removed. However, they can be helpful for comparing the actual inflows between different run configurations.\n\nImportantly, as a special requirement, we have to store the references to the coupling constraints in subproblem.ext[:coupling_constraints], as this is required in the cut computation within dual.jl.\n\nAs we deal with a minimization problem, as usual in SDDP.jl, we specify a valid lower bound in the PolicyGraph definition.\n\nNOTE: In a very similar way, the hydrothermal model for running SDDP.jl with data from a linearized AR process is defined in hydrothermal_model_linearized.jl. The main differences are (a) that the state space has to be expanded (note that inflow is now a state variable) and (b) that the parameterize statement has to be adjusted for the linearized process.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/paper_results/#Results-from-our-experiments","page":"Results from our experiments","title":"Results from our experiments","text":"","category":"section"},{"location":"example/paper_results/#Result-data","page":"Results from our experiments","title":"Result data","text":"The full results from our computational experiments are available in the folder Computational_Results. For a description on how to interpret the output files, see section logging.","category":"section"},{"location":"example/paper_results/#Reproducing-the-results","page":"Results from our experiments","title":"Reproducing the results","text":"To reproduce the results from our paper, it is simply required to run the files run_model.jl, run_model_linearized.jl and run_model_markov.jl on the current main branch.\n\nNote that for loglinearSDDP, cut selection is not supported yet, so one run of run_model.jl is sufficient. For the other two variants, we executed two batches of runs, one with and one without cut selection. The default case is that SDDP.jl is run with cut selection in both run_model_linearized.jl and run_model_markov.jl.\n\nUnfortunately, the train function in SDDP.jl does not have an argument to control the cut selection scheme. Therefore, if run_model_linearized.jl and run_model_markov.jl should be run without cut selection, it is required to develop the SDDP.jl package and to manually switch the cut_selection parameter from True to False in the file bellman_functions.jl for function add_cut.jl.\n\nKeep in mind that we used Gurobi 11 and older versions of JuMP.jl, SDDP.jl and several other packages when performing our experiments, so the results that you obtain may slightly deviate from what we report. The same can be true if the code is executed on a different hardware than ours.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"algorithm/#Implementation-of-the-loglinearSDDP-algorithm","page":"Implementation of the loglinearSDDP algorithm","title":"Implementation of the loglinearSDDP algorithm","text":"The implementation of loglinearSDDP is contained in the src folder and consists of the following files:\n\n*\talgorithm.jl:     *\tContains the main functions from (an earlier version of) SDDP.jl such as train, backward_pass, solve_subproblem etc. that are required for SDDP to work.     *\tIncludes a few adjustments to our specific version of SDDP.     *\tIn parameterize it is made sure that the cut intercepts are re-evaluated for a given scenario.     *\tBefore calling the train method, the train_loglinear function is called to initialize the history of the stochastic process, to compute the cut exponents Theta (see cut representation), adjust the value functions (Bellman functions) to our needs etc.\n\n*\tar_preparations.jl:     *\tContains function initialize_process_state which uses the history from the AutoregressiveProcess struct to set up the initial state of the stochastic process. This state is then used as an input for following stages.     *\tIf not enough history is provided by the user, based on the maximum dimension L and the lag order p some default history will be created by the code.\n\n*\tbellman.jl and bellman_redefine.jl:     *\tContains the functionality of the value functions (Bellman functions) that are approximated within SDDP, mostly borrowed from SDDP.jl     *\tIncludes a few adjustments for our case, e.g. making sure that the code works with our extended Cut structs and our specific cut formulas (see cut representation).\n\n*\tcut_computations.jl     *\tContains most of the computations that are related to our special version of SDDP     *\tcompute_cut_exponents computes the exponents Theta that are used in the cut formulas (see cut representation). They only have to be computed once in advance before the iteration loop in SDDP is started. Note that, compared to the paper, the indexing is a bit different. Precisely, theta(ttauellmk) from the paper (with k a stage) translates to cut_exponents_stage[t][τ,ℓ,m,κ] (with κ the lag and κ = t-k).     *\tevaluate_cut_intercepts updates the existing cut intercepts to a given scenario.     *\tevaluate_stochastic_cut_intercept_tight evaluates the cut intercept at the incumbent (including the current process state) where it is constructed.     *\tupdate_process_state updates the state of the stochastic process for a specific realization that was sampled. The current process state is always stored in node.ext[:process_state].     *\tcompute_scenario_factors computes the scenario-specific factors prod_k=t-p^t-1 prod_m=1^L_k xi_km^Theta(ttauellmk) that are required in the cut intercept formula (see cut representation). They are the same for all cuts.     *\tadapt_intercepts iterates over all existing cuts to adjust the intercepts. To this end, first the final intercept value is computed using the scenario_factors and then cut_intercept_variable is fixed to this value.\n\n*\tduals.jl:     *\tContains the backward pass functionality of SDDP to compute optimal dual multipliers / cut coefficients. Compared to SDDP.jl this is enhanced a lot to be tailored to our special version of SDDP. In particular, the cut intercept factors alpha that are required in the cut formulas (see cut representation) are computed.     *\tget_alpha controls the process of computing these factors alpha.     *\tcompute_alpha_t computes the value of alpha^(t).     *\tcompute_alpha_tau computes the value alpha^(tau). If simplified is set to True in the AutoregressiveProcess struct, then a simplified version of this computation can be applied to accelerate the solution process     *\tget_existing_cut_factors computes the first factor sum_r in R_t+1 rho^*_rtj alpha_rt+1ell^(tau) required for the computation of alpha. It requires to iterate over all previously generated cuts and to multiply the corresponding dual multiplier with a cut intercept factor of that cut.\n\n*\tlogging.jl: Controls (most of) the logging of SDDP.\n\n*\tsampling_schemes.jl:     *\tContains functionality for the correct sampling of scenarios within SDDP.     *\tFunction sample_scenario is taken from SDDP.jl, but adjusted to make sure that it fits log-linear AR processes. It computes a new realization of xi_t as required in the forward pass. To to dis, it is first sampled only from the stagewise independent process eta_t using functionality from SDDP.jl. Using the current state of the process and the process formula, then the new value of xi_t is computed (see formula (15) in the paper).     *\tupdate_process_state uses the new realization to update the state of the process for the following stage.     *\tsample_backward_noise_terms is the analogue to sample_scenario but for the backward pass of SDDP.\n\n*\tsimulate.jl: Takes the simulation functionality from SDDP.jl and adjusts it to fit our purposes, e.g. by using our tailored sample_scenario function.\n\n*\ttypedefs.jl: Contains the definition of the ProblemParams, AlgoParams, AutoregressiveProcess and AutoregressiveProcessStage structs discussed above.\n\nNote that we have used packages Tullio.jl and LoopVectorization.jl (with macro @turbo) to speed up the computations   within cut_computations.jl and duals.jl as much and keep the overhead to standard SDDP as small as possible.\n\nNote that we have also implemented some Gurobi-specific methods, e.g. to accelerate the variable fixing or dual muliplier evaluation. These variants can only be used if the correct Gurobi-internal indices for the cut intercept variable, the coupling constraints or the cut constraints are provided using parameters gurobi_fix_start gurobi_coupling_index_start and  gurobi_cut_index_start. For our hydrothermal scheduling problem (see [TODO]) these values have been identified in advance.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"example/fitting_process/#Model-fitting-and-data-preparation","page":"Model fitting and data preparation","title":"Model fitting and data preparation","text":"The folders PreparationAutoregressive and PreparationAutoregressiveLinearized contain files for the preparation of the AR processes and inflow realizations if either a log-linear model or a linearized model is used. In particular, they contain code to fit the models LOG-BIC, LOG-1 and LIN-FIT that we used in our experiments, whereas for LIN-SHA data from the literature is used.\n\nFor details on the methodology that is used, we refer to the electronic companion supplementing our paper. Here, we simply focus on explaining the structure of the code. We restrict to PreparationAutoregressive. For PreparationAutoregressiveLinearized the structure is very similar.\n\nNOTE: If you simply want to reproduce the SDDP results given the pre-fitted AR models that we used, then you do not have to deal with these folders at all.\n\nThe PreparationAutoregressive contains the following files: *\trun_file.jl: Using this file, we can run the preprocessing of the loglinear AR models. *\tAutoregressivePreparation.jl: Contains the module definition; also defines a MonthlyModelStorage struct in which information is stored during the fitting process *\tdata_plots.jl: Contains various functions for creating plots, e.g. plotting the original or detrended inflow data, plotting (partial) autocorrelation functions, creating boxplots for model validation *\tdata_preparation.jl: Contains functionality to read the historical inflow data (which is stored in files hist1.csv, hist2.csv, hist3.csv and hist4.csv in folder historical_data), to detrend it and split it into training and validation sets. *\tdata_analysis.jl: Contains a function to compute the sample autocovariance for a given time series (in dataframe format) for all months and lags. This is required to compute periodical AFCs, for instance. *\tbox_jenkins.jl: Contains functionality to perform the Box-Jenkins method to fit an AR model to the time series data. The steps are     *\tAnalyzing the (P)AFC to detect autocorrelation     *\tPerform an augmented Dickey-Fuller test to test for stationarity     *\tFit an AR model to the time series     *\tUse the fitted model and the residuals for validations (analyzing the goodness of fit, performing significance tests, analyzing autocorrelation and heteroscedasticity in the residuals, testing for normal distribution of the residuals). *\tperiodic_box_jenkins.jl: Similar to boxjenkins.jl but more advanced as it allows us to fit periodic models were the coefficients differ between months. This is used to fit the models that we use within SDDP.     * \tThe lag order can either be identified by using periodic AFCs or by fitting several models with different lag order (from 1 to 12) and choosing the lag order that yields the best AIC or BIC measure. *\tforecasting.jl: Contains functionality to create point forecasts as well as full scenarios (over the whole time horizon) using the fitted models. This is done for further model validation *\t`armodelgeneration.jl: Main file for preparing the AR model data, as it uses functions from all the previous files. The procedure works as follows:     *\tIterate over all 4 reservoirs     *\tRead the relevant data     *\tAnalyze the data using plots     *\tConvert it to logarithmic data     *\tDetrend the data and split it into training and validation data     *\tPerform a (periodic) Box-Jenkins analysis to fit an AR model to the data and validate it.     *\tCreate point forecasts using the fitted model for further model validation.     *\tCreate full scenarios (over the whole time horizon) using the fitted model for further model validation.     *\tPrepare the model output in the correct form so that it can be used as an input in our version of SDDP as well as out-of-sample simulations. Also checks are performed to assert that this is done correctly. *scenariogeneration.jl:     *\tOncearmodelgeneration.jlis finished and the process coefficients are stored correctly, this file can be used to generate a predefined number of realizations for the stagewise independent term in the AR model. These realizations are then used inhydrothermal_model.jl` to parameterize the uncertain data.     *\tAdditionally, this file can be used to prepare the history of the stochastic process that is required in our version of SDDP.\n\nBoth ar_model_generation.jl (executed using run-file.jl) and scenario_generation.jl were executed before running our experiments in SDDP. Once we started our experiments in SDDP, all the inputs were fixed.\n\nThe folder PreparationHistorical contains files to prepare the historical data for the out-of-sample simulations.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#An-introduction-to-LogLinearSDDP.jl","page":"Home","title":"An introduction to LogLinearSDDP.jl","text":"This code implements a special version of stochastic dual dynamic programming (SDDP) and is based on (an earlier version of) SDDP.jl from Oscar Dowson. The special version of SDDP, which we refer to as loglinearSDDP, is tailored to solve multistage stochastic linear problems with log-linear autoregressive uncertainty in the right-hand side. By this we mean that the uncertain data in the right-hand side is stagewise dependent and described by an autoregressive process that is linear in the natural logarithm of its lagged variables.\n\n\n\nThis page was generated using Literate.jl.","category":"section"}]
}
